{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(\n",
    "            self,\n",
    "            patch_size = 8,\n",
    "            projection_dim = 64,\n",
    "            image_size = 256,\n",
    "            eps = 1e-6,\n",
    "            channels = 3,\n",
    "            num_heads = 2,\n",
    "            dropout = 0.5,\n",
    "            intermediate_size = 128,\n",
    "            num_hidden_layers=12,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()      \n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.image_size = image_size\n",
    "        self.eps = eps\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*8*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches:  torch.Size([1, 1024, 192])\n",
      "patch_encoder:  torch.Size([1, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "\n",
    "patch_size = 8\n",
    "projection_dim = 64\n",
    "IMG_SIZE = 256\n",
    "eps=1e-6\n",
    "\n",
    "class Patches:\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        self.config = config\n",
    "        self.patch_size = config.patch_size\n",
    "    def __call__(self, images):\n",
    "        # Input: [Batch_Size, Channels, Height, Width]\n",
    "        # Output: [Batch_Size, Patch_Dim, Num_Patches]\n",
    "        # Num_Patches = (Height//Patch_Size) * (Width//Patch_Size)\n",
    "        # Patch_Dims = Channels * Patch_Size * Patch_Size\n",
    "        assert (images.shape[2] >= patch_size) and (images.shape[3] >= patch_size), f\"Image size: {images.shape} must be bigger or equal to patch size: {patch_size}\"\n",
    "        patches = F.unfold(images, \n",
    "                           kernel_size=self.patch_size, \n",
    "                           stride=self.patch_size)\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.num_patches = (config.image_size//config.patch_size)**2\n",
    "        self.projection = nn.Linear(config.patch_size * config.patch_size * config.channels, config.projection_dim)\n",
    "        self.position_embedding = nn.Embedding(self.num_patches, config.projection_dim)\n",
    "    def forward(self, patch):\n",
    "        positions = torch.arange(start=0, end = self.num_patches, device=patches.device)\n",
    "        a = self.projection(patch)\n",
    "        b = self.position_embedding(positions)\n",
    "        encoded = a + b\n",
    "        return encoded\n",
    "        \n",
    "img = torch.rand(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "p = Patches(config=TransformerConfig())\n",
    "patches = p(img).transpose(1,2) # [Batch_Size, Num_Patches, Patch_Dim]\n",
    "print(\"patches: \", patches.shape)\n",
    "\n",
    "pe = PatchEncoder(config=TransformerConfig())\n",
    "patch_encoder = pe(patches)\n",
    "print(\"patch_encoder: \",patch_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 64])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.projection_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5 # 1/root(head_dim)\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(in_features = self.embed_dim, out_features = self.embed_dim)\n",
    "        self.v_proj = nn.Linear(in_features = self.embed_dim, out_features = self.embed_dim)\n",
    "        self.q_proj = nn.Linear(in_features = self.embed_dim, out_features = self.embed_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(in_features = self.embed_dim, out_features = self.embed_dim)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch. Tensor, torch. Tensor]:\n",
    "        batch_size, seq_len, embed_dim = hidden_states.size() # seq_len is same as Num_Patches and embed_dim is same as projection or hdden size\n",
    "\n",
    "        query_states = self.q_proj(hidden_states) # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        key_states = self.k_proj(hidden_states)   # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        value_states = self.v_proj(hidden_states) # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2) # [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)     # [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2) # [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "\n",
    "        # [Batch_Size, Num_Heads, Num_Patches, Head_Dim] * [Batch_Size, Num_Heads, Head_Dim, Num_Patches] -> [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attn_weights = (torch.matmul(query_states, key_states.transpose(2,3)) * self.scale)\n",
    "\n",
    "        # [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        #[Batch_Size, Num_Heads, Num_Patches, Num_Patches] * [Batch_Size, Num_Heads, Num_Patches, Head_Dim] = \n",
    "        attn_output = torch.matmul(attn_weights, value_states) # [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        \n",
    "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
    "            raise ValueError(f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)} but is {attn_weights.size()}\")\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().reshape(batch_size, seq_len, self.embed_dim) # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        attn_output = self.out_proj(attn_output) # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(config.projection_dim, config.intermediate_size)\n",
    "        self.layer_2 = nn.Linear(config.intermediate_size, config.projection_dim)\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "        hidden_states = self.layer_1(hidden_states)\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        return self.layer_2(hidden_states)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embeds = config.projection_dim\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(self.embeds, eps=config.eps)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(self.embeds, eps=config.eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm_1(hidden_states)\n",
    "        hidden_states, _ = self.attn(hidden_states)\n",
    "        hidden_states += residual\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm_2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states += residual\n",
    "        return hidden_states\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "    def forward(self, hidden_states):\n",
    "        for encoder_layer in self.layers:\n",
    "            hidden_states = encoder_layer(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "# mha = MultiHeadAttention(TransformerConfig())\n",
    "# attn_output, attn_weights = mha(patch_encoder)\n",
    "# attn_output.shape, attn_weights.shape\n",
    "TransformerConfig().num_heads = 2\n",
    "\n",
    "el = TransformerEncoder(TransformerConfig())\n",
    "output = el(patch_encoder)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  401,664\n"
     ]
    }
   ],
   "source": [
    "parameters = sum(p.numel() for p in TransformerEncoder(TransformerConfig()).parameters())\n",
    "print(\"Total parameters: \", \"{:,}\".format(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu_bn(inputs: torch.Tensor) -> torch.Tensor:\n",
    "    relu = nn.functional.relu(inputs)\n",
    "    bn = nn.BatchNorm2d(num_features=inputs.shape[1], eps=TransformerConfig().eps)(relu)\n",
    "    return bn\n",
    "\n",
    "def residual_block(x: torch.Tensor, downsample: bool, out_channels: int, kernel_size: int = 3) -> torch.Tensor:\n",
    "    in_channels = x.shape[1]\n",
    "    y = nn.Conv2d(\n",
    "        in_channels=in_channels,  # Specify the correct number of input channels\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=(1 if not downsample else 2),\n",
    "        padding=kernel_size // 2, \n",
    "        bias=True  \n",
    "    )(x)\n",
    "\n",
    "    y = relu_bn(y)\n",
    "    y = nn.Conv2d(\n",
    "        in_channels=y.shape[1],  \n",
    "        out_channels=out_channels,  \n",
    "        kernel_size=kernel_size,\n",
    "        stride=1,\n",
    "        padding=kernel_size // 2  \n",
    "    )(y)\n",
    "\n",
    "    if downsample:\n",
    "        x = nn.Conv2d(\n",
    "            in_channels=x.shape[1],  \n",
    "            out_channels=out_channels,  \n",
    "            kernel_size=1,\n",
    "            stride=2,\n",
    "            padding=0  \n",
    "        )(x)\n",
    "    return relu_bn(x+y)\n",
    "\n",
    "\n",
    "# relu_bn(torch.rand([1, 512, 4, 4])).shape\n",
    "residual_block(torch.rand([1, 512, 4, 4]), downsample=False, out_channels=512).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = output.view(output.shape[0], 1024, 8, 8)\n",
    "x = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=6, stride=2, padding=2, bias=False)(x)\n",
    "x = nn.BatchNorm2d(num_features=x.shape[1], eps=TransformerConfig().eps)(x)\n",
    "x = nn.functional.leaky_relu(x)\n",
    "\n",
    "x = residual_block(x, downsample=False, out_channels=512)\n",
    "\n",
    "x = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=6, stride=2, padding=2, bias=False)(x)\n",
    "x = nn.BatchNorm2d(num_features=x.shape[1], eps=TransformerConfig().eps)(x)\n",
    "x = nn.functional.leaky_relu(x)\n",
    "\n",
    "x = residual_block(x, downsample=False, out_channels=256)\n",
    "\n",
    "x = nn.ConvTranspose2d(in_channels=256, out_channels=64, kernel_size=7, stride=2, padding=2, bias=False)(x)\n",
    "x = nn.BatchNorm2d(num_features=x.shape[1], eps=TransformerConfig().eps)(x)\n",
    "x = nn.functional.leaky_relu(x)\n",
    "\n",
    "x = residual_block(x, downsample=False, out_channels=64)\n",
    "\n",
    "x = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=7, stride=4, padding=2, bias=False)(x)\n",
    "x = nn.BatchNorm2d(num_features=x.shape[1], eps=TransformerConfig().eps)(x)\n",
    "x = nn.functional.leaky_relu(x)\n",
    "\n",
    "x = residual_block(x, downsample=False, out_channels=32)\n",
    "\n",
    "x = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=6, stride=1, padding=1, bias=False)(x)\n",
    "\n",
    "x = torch.tanh(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4079, -0.5124, -0.3488,  1.0264],\n",
       "          [-1.0085, -0.9503, -0.1341, -1.0202],\n",
       "          [ 0.5932,  0.2505,  1.1492, -0.2031],\n",
       "          [ 0.2689,  0.3259, -0.3090,  0.0879]],\n",
       "\n",
       "         [[-0.1576,  0.4141, -0.3625,  0.0534],\n",
       "          [-0.8727, -0.5027, -0.3476, -0.2474],\n",
       "          [ 1.0394,  0.7016,  1.0936, -0.7991],\n",
       "          [ 0.4022, -0.5924,  0.1934,  0.4014]],\n",
       "\n",
       "         [[ 0.4922,  0.8760,  0.6603, -0.2504],\n",
       "          [-0.3886, -0.7083, -0.2488,  0.5749],\n",
       "          [ 0.0527, -0.6892, -0.4158, -2.0652],\n",
       "          [ 0.2956, -0.7855, -0.3119, -0.9370]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7177,  0.6518,  1.1904,  0.4109],\n",
       "          [-0.4125,  0.8926,  0.4125,  0.9007],\n",
       "          [-0.1033,  1.5577, -0.0272, -0.0751],\n",
       "          [ 0.0894,  0.7837, -0.1577,  0.1917]],\n",
       "\n",
       "         [[-1.0882, -0.5651, -0.1209, -0.2276],\n",
       "          [-0.2136, -0.8643,  0.3905,  1.6802],\n",
       "          [ 0.7052, -0.5940,  1.6069,  0.2180],\n",
       "          [-0.0748, -0.8118, -0.7628, -0.3698]],\n",
       "\n",
       "         [[ 0.1652,  0.0655,  0.9528, -0.2164],\n",
       "          [-0.1523, -0.0280, -0.9567, -0.3365],\n",
       "          [ 0.2818,  0.5975, -0.4649,  0.1323],\n",
       "          [-0.3708,  0.1861, -0.0910,  0.6904]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=5, stride=2, padding=2, bias=False)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 1024])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (25) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m projection_dim \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Use the correct projection dimension\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pe \u001b[38;5;241m=\u001b[39m PatchEncoder(num_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, projection_dim\u001b[38;5;241m=\u001b[39mprojection_dim)  \u001b[38;5;66;03m# Correct initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m encoded_patches \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Encode patches\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_patches\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[106], line 13\u001b[0m, in \u001b[0;36mPatchEncoder.forward\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, patches):\n\u001b[1;32m     12\u001b[0m     positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_patches, device\u001b[38;5;241m=\u001b[39mpatches\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (25) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = nn.Linear(in_features=projection_dim, out_features=projection_dim)\n",
    "        self.position_embedding = nn.Embedding(num_patches, projection_dim)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        positions = torch.arange(self.num_patches, device=patches.device)\n",
    "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# Example usage\n",
    "img = torch.rand(1, 3, 5, 5)  # Example input image\n",
    "p = Patches(1)  # Ensure Patches is implemented correctly\n",
    "patches = p(img)  # Extract patches\n",
    "\n",
    "projection_dim = patches.shape[-1]  # Use the correct projection dimension\n",
    "pe = PatchEncoder(num_patches=25, projection_dim=projection_dim)  # Correct initialization\n",
    "encoded_patches = pe(patches)  # Encode patches\n",
    "\n",
    "print(encoded_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 25])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 5, 5)\n",
    "p = Patches(1)\n",
    "patches = p(img)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
